#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DeepSentimentCrawlingæ¨¡å— - ä¸»å·¥ä½œæµç¨‹
åŸºäºBroadTopicExtractionextractçš„topicè¿›è¡Œå…¨platformkeywordcrawl
"""

import sys
import argparse
from datetime import date, datetime
from pathlib import Path
from typing import List, Dict

# Add project root directory to path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from keyword_manager import KeywordManager
from platform_crawler import PlatformCrawler

class DeepSentimentCrawling:
    """æ·±åº¦sentiment crawlingä¸»å·¥ä½œæµç¨‹"""
    
    def __init__(self):
        """initializeæ·±åº¦sentiment crawling"""
        self.keyword_manager = KeywordManager()
        self.platform_crawler = PlatformCrawler()
        self.supported_platforms = ['xhs', 'dy', 'ks', 'bili', 'wb', 'tieba', 'zhihu']
    
    def run_daily_crawling(self, target_date: date = None, platforms: List[str] = None, 
                          max_keywords_per_platform: int = 50, 
                          max_notes_per_platform: int = 50,
                          login_type: str = "qrcode") -> Dict:
        """
        executeæ¯æ—¥crawltask
        
        Args:
            target_date: ç›®æ ‡æ—¥æœŸï¼Œdefaultä¸ºä»Šå¤©
            platforms: è¦crawlçš„platformåˆ—tableï¼Œdefaultä¸ºallæ”¯æŒçš„platform
            max_keywords_per_platform: æ¯ä¸ªplatformmaximumkeywordæ•°é‡
            max_notes_per_platform: æ¯ä¸ªplatformmaximumcrawlcontentæ•°é‡
            login_type: ç™»å½•æ–¹å¼
        
        Returns:
            crawlç»“æœstatistics
        """
        if not target_date:
            target_date = date.today()
        
        if not platforms:
            platforms = self.supported_platforms
        
        print(f"ğŸš€ å¼€å§‹execute {target_date} çš„æ·±åº¦sentiment crawlingtask")
        print(f"ç›®æ ‡platform: {platforms}")
        
        # 1. getkeywordæ‘˜è¦
        summary = self.keyword_manager.get_crawling_summary(target_date)
        print(f"ğŸ“Š keywordæ‘˜è¦: {summary}")
        
        if not summary['has_data']:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°topicdataï¼Œunable toè¿›è¡Œcrawl")
            return {"success": False, "error": "æ²¡æœ‰topicdata"}
        
        # 2. getkeywordï¼ˆä¸åˆ†é…ï¼Œallplatformä½¿ç”¨ç›¸åŒkeywordï¼‰
        print(f"\nğŸ“ getkeyword...")
        keywords = self.keyword_manager.get_latest_keywords(target_date, max_keywords_per_platform)
        
        if not keywords:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°keywordï¼Œunable toè¿›è¡Œcrawl")
            return {"success": False, "error": "æ²¡æœ‰keyword"}
        
        print(f"   getåˆ° {len(keywords)} keyword")
        print(f"   å°†åœ¨ {len(platforms)} platformä¸Šcrawlæ¯ä¸ªkeyword")
        print(f"   æ€»crawltask: {len(keywords)} Ã— {len(platforms)} = {len(keywords) * len(platforms)}")
        
        # 3. executeå…¨platformkeywordcrawl
        print(f"\nğŸ”„ å¼€å§‹å…¨platformkeywordcrawl...")
        crawl_results = self.platform_crawler.run_multi_platform_crawl_by_keywords(
            keywords, platforms, login_type, max_notes_per_platform
        )
        
        # 4. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        final_report = {
            "date": target_date.isoformat(),
            "summary": summary,
            "crawl_results": crawl_results,
            "success": crawl_results["successful_tasks"] > 0
        }
        
        print(f"\nâœ… æ·±åº¦sentiment crawlingtaskcompleted!")
        print(f"   æ—¥æœŸ: {target_date}")
        print(f"   successtask: {crawl_results['successful_tasks']}/{crawl_results['total_tasks']}")
        print(f"   æ€»keyword: {crawl_results['total_keywords']} ")
        print(f"   æ€»platform: {crawl_results['total_platforms']} ")
        print(f"   æ€»content: {crawl_results['total_notes']} ")
        
        return final_report
    
    def run_platform_crawling(self, platform: str, target_date: date = None,
                             max_keywords: int = 50, max_notes: int = 50,
                             login_type: str = "qrcode") -> Dict:
        """
        executeå•ä¸ªplatformçš„crawltask
        
        Args:
            platform: platformåç§°
            target_date: ç›®æ ‡æ—¥æœŸ
            max_keywords: maximumkeywordæ•°é‡
            max_notes: maximumcrawlcontentæ•°é‡
            login_type: ç™»å½•æ–¹å¼
        
        Returns:
            crawlç»“æœ
        """
        if platform not in self.supported_platforms:
            raise ValueError(f"ä¸æ”¯æŒçš„platform: {platform}")
        
        if not target_date:
            target_date = date.today()
        
        print(f"ğŸ¯ å¼€å§‹execute {platform} platformçš„crawltask ({target_date})")
        
        # getkeyword
        keywords = self.keyword_manager.get_keywords_for_platform(
            platform, target_date, max_keywords
        )
        
        if not keywords:
            print(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ° {platform} platformçš„keyword")
            return {"success": False, "error": "æ²¡æœ‰keyword"}
        
        print(f"ğŸ“ å‡†å¤‡crawl {len(keywords)} keyword")
        
        # executecrawl
        result = self.platform_crawler.run_crawler(
            platform, keywords, login_type, max_notes
        )
        
        return result
    
    def list_available_topics(self, days: int = 7):
        """åˆ—å‡ºæœ€è¿‘å¯ç”¨çš„topic"""
        print(f"ğŸ“‹ æœ€è¿‘ {days} daysçš„topicdata:")
        
        recent_topics = self.keyword_manager.db_manager.get_recent_topics(days)
        
        if not recent_topics:
            print("   æš‚æ— topicdata")
            return
        
        for topic in recent_topics:
            extract_date = topic['extract_date']
            keywords_count = len(topic.get('keywords', []))
            summary_preview = topic.get('summary', '')[:100] + "..." if len(topic.get('summary', '')) > 100 else topic.get('summary', '')
            
            print(f"   ğŸ“… {extract_date}: {keywords_count} keyword")
            print(f"      æ‘˜è¦: {summary_preview}")
            print()
    
    def show_platform_guide(self):
        """showplatformä½¿ç”¨æŒ‡å—"""
        print("ğŸ”§ platformcrawlæŒ‡å—:")
        print()
        
        platform_info = {
            'xhs': 'Xiaohongshu - ç¾å¦†ã€ç”Ÿæ´»ã€æ—¶å°šcontentä¸ºä¸»',
            'dy': 'Douyin - çŸ­è§†é¢‘ã€å¨±ä¹ã€ç”Ÿæ´»content',
            'ks': 'Kuaishou - ç”Ÿæ´»ã€å¨±ä¹ã€å†œæ‘é¢˜æcontent',
            'bili': 'Bilibili - ç§‘æŠ€ã€å­¦ä¹ ã€æ¸¸æˆã€åŠ¨æ¼«content',
            'wb': 'Weibo - çƒ­ç‚¹newsã€æ˜æ˜Ÿã€ç¤¾ä¼štopic',
            'tieba': 'ç™¾åº¦Tieba - å…´è¶£è®¨è®ºã€æ¸¸æˆã€å­¦ä¹ ',
            'zhihu': 'Zhihu - çŸ¥è¯†é—®ç­”ã€æ·±åº¦è®¨è®º'
        }
        
        for platform, desc in platform_info.items():
            print(f"   {platform}: {desc}")
        
        print()
        print("ğŸ’¡ ä½¿ç”¨å»ºè®®:")
        print("   1. é¦–æ¬¡ä½¿ç”¨needæ‰«ç ç™»å½•å„platform")
        print("   2. å»ºè®®å…ˆæµ‹è¯•å•ä¸ªplatformï¼Œç¡®è®¤ç™»å½•normal")
        print("   3. crawlæ•°é‡ä¸å®œè¿‡å¤§ï¼Œé¿å…è¢«é™åˆ¶")
        print("   4. å¯ä»¥ä½¿ç”¨ --test æ¨¡å¼è¿›è¡Œå°è§„æ¨¡æµ‹è¯•")
    
    def close(self):
        """å…³é—­èµ„æº"""
        if self.keyword_manager:
            self.keyword_manager.close()

def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(description="DeepSentimentCrawling - åŸºäºtopicçš„æ·±åº¦sentiment crawling")
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument("--date", type=str, help="ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)ï¼Œdefaultä¸ºä»Šå¤©")
    parser.add_argument("--platform", type=str, choices=['xhs', 'dy', 'ks', 'bili', 'wb', 'tieba', 'zhihu'], 
                       help="æŒ‡å®šå•ä¸ªplatformè¿›è¡Œcrawl")
    parser.add_argument("--platforms", type=str, nargs='+', 
                       choices=['xhs', 'dy', 'ks', 'bili', 'wb', 'tieba', 'zhihu'],
                       help="æŒ‡å®šå¤šä¸ªplatformè¿›è¡Œcrawl")
    
    # crawlå‚æ•°
    parser.add_argument("--max-keywords", type=int, default=50, 
                       help="æ¯ä¸ªplatformmaximumkeywordæ•°é‡ (default: 50)")
    parser.add_argument("--max-notes", type=int, default=50,
                       help="æ¯ä¸ªplatformmaximumcrawlcontentæ•°é‡ (default: 50)")
    parser.add_argument("--login-type", type=str, choices=['qrcode', 'phone', 'cookie'], 
                       default='qrcode', help="ç™»å½•æ–¹å¼ (default: qrcode)")
    
    # åŠŸèƒ½å‚æ•°
    parser.add_argument("--list-topics", action="store_true", help="åˆ—å‡ºæœ€è¿‘çš„topicdata")
    parser.add_argument("--days", type=int, default=7, help="æŸ¥çœ‹æœ€è¿‘å‡ å¤©çš„topic (default: 7)")
    parser.add_argument("--guide", action="store_true", help="showplatformä½¿ç”¨æŒ‡å—")
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•æ¨¡å¼ (å°‘é‡data)")
    
    args = parser.parse_args()
    
    # è§£ææ—¥æœŸ
    target_date = None
    if args.date:
        try:
            target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
        except ValueError:
            print("âŒ æ—¥æœŸæ ¼å¼errorï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
            return
    
    # åˆ›å»ºcrawlå®ä¾‹
    crawler = DeepSentimentCrawling()
    
    try:
        # showæŒ‡å—
        if args.guide:
            crawler.show_platform_guide()
            return
        
        # åˆ—å‡ºtopic
        if args.list_topics:
            crawler.list_available_topics(args.days)
            return
        
        # æµ‹è¯•æ¨¡å¼è°ƒæ•´å‚æ•°
        if args.test:
            args.max_keywords = min(args.max_keywords, 10)
            args.max_notes = min(args.max_notes, 10)
            print("æµ‹è¯•æ¨¡å¼ï¼šé™åˆ¶keywordå’Œcontentæ•°é‡")
        
        # å•platformcrawl
        if args.platform:
            result = crawler.run_platform_crawling(
                args.platform, target_date, args.max_keywords, 
                args.max_notes, args.login_type
            )
            
            if result['success']:
                print(f"\n{args.platform} crawlsuccessï¼")
            else:
                print(f"\n{args.platform} crawlfailed: {result.get('error', 'æœªçŸ¥error')}")
            
            return
        
        # å¤šplatformcrawl
        platforms = args.platforms if args.platforms else None
        result = crawler.run_daily_crawling(
            target_date, platforms, args.max_keywords, 
            args.max_notes, args.login_type
        )
        
        if result['success']:
            print(f"\nå¤šplatformcrawltaskcompletedï¼")
        else:
            print(f"\nå¤šplatformcrawlfailed: {result.get('error', 'æœªçŸ¥error')}")
    
    except KeyboardInterrupt:
        print("\nuserä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nexecuteå‡ºé”™: {e}")
    finally:
        crawler.close()

if __name__ == "__main__":
    main()

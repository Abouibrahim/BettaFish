"""
Keyword Optimization Middleware
Uses Qwen AI to optimize Agent-generated search terms into keywords more suitable for public opinion database queries
"""

from openai import OpenAI
import json
import sys
import os
from typing import List, Dict, Any
from dataclasses import dataclass

# Add project root to Python path for importing config
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
from config import settings
from loguru import logger

# Add utils directory to Python path
current_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(os.path.dirname(current_dir))
utils_dir = os.path.join(root_dir, 'utils')
if utils_dir not in sys.path:
    sys.path.append(utils_dir)

from retry_helper import with_graceful_retry, SEARCH_API_RETRY_CONFIG

@dataclass
class KeywordOptimizationResponse:
    """Keyword optimization response"""
    original_query: str
    optimized_keywords: List[str]
    reasoning: str
    success: bool
    error_message: str = ""

class KeywordOptimizer:
    """
    Keyword Optimizer
    Uses SiliconFlow's Qwen3 model to optimize Agent-generated search terms into keywords closer to real public opinion
    """

    def __init__(self, api_key: str = None, base_url: str = None, model_name: str = None):
        """
        Initialize keyword optimizer

        Args:
            api_key: SiliconFlow API key, reads from config file if not provided
            base_url: API base URL, defaults to SiliconFlow address from config file
        """
        self.api_key = api_key or settings.KEYWORD_OPTIMIZER_API_KEY

        if not self.api_key:
            raise ValueError("SiliconFlow API key not found, please set KEYWORD_OPTIMIZER_API_KEY in config.py")

        self.base_url = base_url or settings.KEYWORD_OPTIMIZER_BASE_URL

        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
        self.model = model_name or settings.KEYWORD_OPTIMIZER_MODEL_NAME
    
    def optimize_keywords(self, original_query: str, context: str = "") -> KeywordOptimizationResponse:
        """
        Optimize search keywords

        Args:
            original_query: Original search query generated by Agent
            context: Additional context information (such as paragraph title, content description, etc.)

        Returns:
            KeywordOptimizationResponse: Optimized keyword list
        """
        logger.info(f"üîç Keyword Optimization Middleware: Processing query '{original_query}'")
        
        try:
            # Build optimization prompt
            system_prompt = self._build_system_prompt()
            user_prompt = self._build_user_prompt(original_query, context)

            # Call Qwen API
            response = self._call_qwen_api(system_prompt, user_prompt)

            if response["success"]:
                # Parse response
                content = response["content"]
                try:
                    # Try parsing JSON format response
                    if content.strip().startswith('{'):
                        parsed = json.loads(content)
                        keywords = parsed.get("keywords", [])
                        reasoning = parsed.get("reasoning", "")
                    else:
                        # If not JSON format, try extracting keywords from text
                        keywords = self._extract_keywords_from_text(content)
                        reasoning = content

                    # Validate keyword quality
                    validated_keywords = self._validate_keywords(keywords)

                    logger.info(
                        f"‚úÖ Optimization successful: {len(validated_keywords)} keywords" +
                        ("" if not validated_keywords else "\n" +
                         "\n".join([f"   {i}. '{k}'" for i, k in enumerate(validated_keywords, 1)]))
                    )
                        
                    
                    
                    return KeywordOptimizationResponse(
                        original_query=original_query,
                        optimized_keywords=validated_keywords,
                        reasoning=reasoning,
                        success=True
                    )
                
                except Exception as e:
                    logger.exception(f"‚ö†Ô∏è Response parsing failed, using fallback: {str(e)}")
                    # Fallback: extract keywords from original query
                    fallback_keywords = self._fallback_keyword_extraction(original_query)
                    return KeywordOptimizationResponse(
                        original_query=original_query,
                        optimized_keywords=fallback_keywords,
                        reasoning="API response parsing failed, using fallback keyword extraction",
                        success=True
                    )
            else:
                logger.error(f"‚ùå API call failed: {response['error']}")
                # Use fallback
                fallback_keywords = self._fallback_keyword_extraction(original_query)
                return KeywordOptimizationResponse(
                    original_query=original_query,
                    optimized_keywords=fallback_keywords,
                    reasoning="API call failed, using fallback keyword extraction",
                    success=True,
                    error_message=response['error']
                )

        except Exception as e:
            logger.error(f"‚ùå Keyword optimization failed: {str(e)}")
            # Final fallback
            fallback_keywords = self._fallback_keyword_extraction(original_query)
            return KeywordOptimizationResponse(
                original_query=original_query,
                optimized_keywords=fallback_keywords,
                reasoning="System error, using fallback keyword extraction",
                success=False,
                error_message=str(e)
            )
    
    def _build_system_prompt(self) -> str:
        """Build system prompt"""
        return """You are a professional public opinion data mining expert. Your task is to optimize user-provided search queries into keywords more suitable for finding in social media public opinion databases.

**Core Principles**:
1. **Close to netizen language**: Use vocabulary that ordinary netizens would use on social media
2. **Avoid professional terminology**: Don't use official words like "public opinion", "propagation", "tendency", "outlook"
3. **Concise and specific**: Each keyword should be very concise and clear for database matching
4. **Emotionally rich**: Include emotional expression vocabulary commonly used by netizens
5. **Quantity control**: Provide minimum 10 keywords, maximum 20 keywords
6. **Avoid repetition**: Don't deviate from the theme of the initial query

**Important Reminder**: Each keyword must be an indivisible independent term, spaces within terms are strictly prohibited. For example, use "Lei Jun class controversy" instead of the incorrect "Lei Jun class controversy" (with space).


**Output Format**:
Please return results in JSON format:
{
    "keywords": ["keyword1", "keyword2", "keyword3"],
    "reasoning": "Reason for choosing these keywords"
}

**Example**:
Input: "Wuhan University public opinion management future outlook development trends"
Output:
{
    "keywords": ["WHU", "Wuhan University", "school management", "university", "education"],
    "reasoning": "Chose 'WHU' and 'Wuhan University' as core vocabulary, these are the most commonly used by netizens; 'school management' is closer to daily expression than 'public opinion management'; avoid using professional terms rarely used by netizens like 'future outlook', 'development trends'"
}"""

    def _build_user_prompt(self, original_query: str, context: str) -> str:
        """Build user prompt"""
        prompt = f"Please optimize the following search query into keywords suitable for public opinion database queries:\n\nOriginal query: {original_query}"

        if context:
            prompt += f"\n\nContext information: {context}"

        prompt += "\n\nRemember: Use vocabulary that netizens actually use on social media, avoid official terms and professional vocabulary."

        return prompt
    
    @with_graceful_retry(SEARCH_API_RETRY_CONFIG, default_return={"success": False, "error": "Keyword optimization service temporarily unavailable"})
    def _call_qwen_api(self, system_prompt: str, user_prompt: str) -> Dict[str, Any]:
        """Call Qwen API"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.7,
            )

            if response.choices:
                content = response.choices[0].message.content
                return {"success": True, "content": content}
            else:
                return {"success": False, "error": "API returned abnormal format"}
        except Exception as e:
            return {"success": False, "error": f"API call exception: {str(e)}"}
    
    def _extract_keywords_from_text(self, text: str) -> List[str]:
        """Extract keywords from text (used when JSON parsing fails)"""
        # Simple keyword extraction logic
        lines = text.split('\n')
        keywords = []

        for line in lines:
            line = line.strip()
            # Find possible keywords
            if 'Ôºö' in line or ':' in line:
                parts = line.split('Ôºö') if 'Ôºö' in line else line.split(':')
                if len(parts) > 1:
                    potential_keywords = parts[1].strip()
                    # Try splitting keywords
                    if '„ÄÅ' in potential_keywords:
                        keywords.extend([k.strip() for k in potential_keywords.split('„ÄÅ')])
                    elif ',' in potential_keywords:
                        keywords.extend([k.strip() for k in potential_keywords.split(',')])
                    else:
                        keywords.append(potential_keywords)

        # If none found, try other methods
        if not keywords:
            # Find content in quotes
            import re
            quoted_content = re.findall(r'["""\'](.*?)["""\']', text)
            keywords.extend(quoted_content)

        # Clean and validate keywords
        cleaned_keywords = []
        for keyword in keywords[:20]:  # Maximum 20
            keyword = keyword.strip().strip('"\'""''')
            if keyword and len(keyword) <= 20:  # Reasonable length
                cleaned_keywords.append(keyword)

        return cleaned_keywords[:20]
    
    def _validate_keywords(self, keywords: List[str]) -> List[str]:
        """Validate and clean keywords"""
        validated = []

        # Bad keywords (too professional or official)
        bad_keywords = {
            'attitude analysis', 'public reaction', 'emotional tendency',
            'future outlook', 'development trends', 'strategic planning', 'policy guidance', 'management mechanism'
        }

        for keyword in keywords:
            if isinstance(keyword, str):
                keyword = keyword.strip().strip('"\'""''')

                # Basic validation
                if (keyword and
                    len(keyword) <= 20 and
                    len(keyword) >= 1 and
                    not any(bad_word in keyword for bad_word in bad_keywords)):
                    validated.append(keyword)

        return validated[:20]  # Return maximum 20 keywords
    
    def _fallback_keyword_extraction(self, original_query: str) -> List[str]:
        """Fallback keyword extraction method"""
        # Simple keyword extraction logic
        # Remove common useless words
        stop_words = {'„ÄÅ'}

        # Split query
        import re
        # Split by spaces and punctuation
        tokens = re.split(r'[\sÔºå„ÄÇÔºÅÔºüÔºõÔºö„ÄÅ]+', original_query)

        keywords = []
        for token in tokens:
            token = token.strip()
            if token and token not in stop_words and len(token) >= 2:
                keywords.append(token)

        # If no valid keywords, use first word of original query
        if not keywords:
            first_word = original_query.split()[0] if original_query.split() else original_query
            keywords = [first_word] if first_word else ["trending"]

        return keywords[:20]

# Global instance
keyword_optimizer = KeywordOptimizer()
